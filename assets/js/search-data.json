{
  
    
        "post0": {
            "title": "Topic Modeling in Python",
            "content": "You developed a mobile app and want to figure out what your users are talking about in the app reviews. You have thousands of tweets mentioning your product and not enough time to read and digest all of them. Maybe you want to look at your emails from the last 5 years and figure out what you have spent your time on while reading and answering them. . If any of these use cases sounds familiar, you should learn about topic modeling! In this article, I will explore various topic modelling algorithms and approaches. You can also open it in Google Colab and apply on your dataset easily! . Install the libraries . To start with, let&#39;s install three libraries: . datasets will allow us to easily grab a bunch of texts to work with | sentence-transformers will help us create text embeddings (more on that later) | bokeh will help us with visualization | . We will install these libraries and import the functions and classes we will need later on. . !pip install -qq datasets !pip install -Uqq sentence-transformers !pip install -qq bokeh . from datasets import load_dataset from sentence_transformers import SentenceTransformer import sklearn.manifold import numpy as np import pandas as pd import random random.seed(42) from bokeh.io import output_file, show from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper from bokeh.palettes import plasma, d3, Turbo256 from bokeh.plotting import figure from bokeh.transform import transform import bokeh.io bokeh.io.output_notebook() import bokeh.plotting as bpl import bokeh.models as bmo bpl.output_notebook() . Grab the data . Topic modeling requires a bunch of texts. We don&#39;t need any labels! Let&#39;s grab an English subset of the public Amazon reviews dataset and test if we can get practical insights on the topics and themes represented in those reviews. . dataset = load_dataset(&#39;amazon_reviews_multi&#39;, &#39;en&#39;) . Reusing dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609) . First Look at the Data . Let&#39;s take a quick look at the data we&#39;ll be working with. Our dataset is a dictionary consisting of three parts: train, validation and test. Let&#39;s peek into the train set and put it into pandas dataframe to see how it&#39;s constructed. . dataset.keys() . dict_keys([&#39;train&#39;, &#39;validation&#39;, &#39;test&#39;]) . df = pd.DataFrame(dataset[&#39;train&#39;]) df.head() . language product_category product_id review_body review_id review_title reviewer_id stars . 0 en | furniture | product_en_0740675 | Arrived broken. Manufacturer defect. Two of th... | en_0964290 | I&#39;ll spend twice the amount of time boxing up ... | reviewer_en_0342986 | 1 | . 1 en | home_improvement | product_en_0440378 | the cabinet dot were all detached from backing... | en_0690095 | Not use able | reviewer_en_0133349 | 1 | . 2 en | home | product_en_0399702 | I received my first order of this product and ... | en_0311558 | The product is junk. | reviewer_en_0152034 | 1 | . 3 en | wireless | product_en_0444063 | This product is a piece of shit. Do not buy. D... | en_0044972 | Fucking waste of money | reviewer_en_0656967 | 1 | . 4 en | pc | product_en_0139353 | went through 3 in one day doesn&#39;t fit correct ... | en_0784379 | bubble | reviewer_en_0757638 | 1 | . This is useful - we can see that the dataset consists of a number of atributes. We&#39;ll focus on the review_body and try to discover topics in those reviews, but the other attributes can help us to validate if we&#39;re stepping in a good direction. For example, we can compare how our topics correlate with the product_category attribute. Let&#39;s peek into the the categories just to see what we have in the dataset. . df.product_category.value_counts().plot(kind=&#39;bar&#39;, figsize=(15,5)); . How can we extract meaning from the review_body though? There are many ways of course. Rather than going bottom up from simple techniques such as key words, n-grams, tf-idf etc., let&#39;s jump straight into the concept of embedding. . Embeddings . A key idea for machine learning is that of representations. Most algorithms can only work with numbers, so whatever we&#39;re dealing with - words, texts, images - we should represent with numbers. We are focusing on texts here, texts can represent many different things, so we also need many numbers - let&#39;s say 768 - for each text. We&#39;ll put these 768 numbers into vectors and use them to represent our texts. These vectors are called embeddings. . For the purpose of these article, we will not worry about where these embeddings come from, other than the fact we can produce them with the SentenceTransformer library. We will load a pretrained model (Distilbert) and use it to encode our texts. . Dimensionality Reduction . 768 numbers for each text is actually less meaningful to a normal person than a text, so how does this help? We can use some magic to reduce these 768 numbers to 2. These magic is called t-SNE and it&#39;s one of several dimensionality reduction techniques (for example PCA or UMAP). It tries to preserve the relative positions of points in a multidimensional space while mapping it to fewer dimensions. With 2 dimensions, we can actually plot these points (texts) on a chart! Let&#39;s do it! . Oh, we have 20.000 texts, so our chart can get really cluttered... Let&#39;s take a 1000 texts sample and use it instead. . model = SentenceTransformer(&#39;stsb-distilbert-base&#39;) . sample = df.sample(n=1000, random_state=42) texts = sample.review_body.values.tolist() categories = sample.product_category.values.tolist() . embeddings = model.encode(texts) . out = sklearn.manifold.TSNE(n_components=2).fit_transform(embeddings) . Visualization with bokeh . Bokeh is a nice tool that allows us to create interactive charts. We&#39;ll use it to create a scatter plot where each text is placed according to the meaning dimension. Additionally, we&#39;re color each dot to indicate which category it comes from. We can hover over the chart and see the text/category associated with each dot. . clrs = random.sample(Turbo256, len(set(categories)), ) color_map = bmo.CategoricalColorMapper(factors=list(set(categories)), palette=clrs) . list_x = out[:,0] list_y = out[:,1] desc = texts source = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, cat=categories)) hover = HoverTool(tooltips=[ (&quot;index&quot;, &quot;$index&quot;), (&quot;(x,y)&quot;, &quot;(@x, @y)&quot;), (&#39;desc&#39;, &#39;@desc&#39;), (&#39;cat&#39;, &#39;@cat&#39;) ]) p = figure(plot_width=1200, plot_height=600, tools=[hover], title=&quot;First Look at the Data&quot;) p.circle(&#39;x&#39;, &#39;y&#39;, size=10, source=source, fill_color=transform(&#39;cat&#39;, color_map),) bpl.show(p) . &lt;!DOCTYPE html&gt; First Look at the Data . . Looks interesting! If you hover over the distinct clusters on the chart, you should be able to recognize common topics. Some of these topics are related to a single category, some of them are shared across categories. What topics can you find in the chart? . Discovering Topics with BERTopic . Looking at the chart above, we can get a sense for some of the topics in our corpus, but it doesn&#39;t solve our problem yet. It would require lots of time to review the chart in detail, find clusters, and label them. How can we automate this process? . BERTopic is one of the methods to achieve that. It depends on sentence embeddings and clustering algorithms, as well as dimensionality reduction to produce clusters of documents (topics). Let&#39;s if we can get some good insights with this approach. . !pip install bertopic -qq . from bertopic import BERTopic . model = BERTopic(language=&quot;english&quot;) topics, probs = model.fit_transform(texts) . len(topics), len(set(topics)) . (1000, 16) . We&#39;ve run the algorithm on our 1000 texts sample, and it identified 16 topics in this corpus. Let&#39;s see if we can learn something more about those topics! . model.get_topic_freq().head(15) . Topic Count . 0 -1 | 451 | . 1 11 | 104 | . 2 9 | 97 | . 3 14 | 61 | . 4 10 | 43 | . 5 1 | 42 | . 6 7 | 28 | . 7 8 | 28 | . 8 0 | 26 | . 9 13 | 22 | . 10 4 | 21 | . 11 5 | 19 | . 12 2 | 16 | . 13 3 | 15 | . 14 6 | 15 | . Wow, there&#39;s quite a lot of outliers here, represented by topic -1, almost half of the dataset! Let&#39;s take a look at one of the topics from this dataset. . model.get_topic(1) . [(&#39;size&#39;, 0.05050109336233438), (&#39;fit&#39;, 0.026139678912211962), (&#39;could&#39;, 0.025590393661103304), (&#39;top&#39;, 0.025448458979747752), (&#39;ordered&#39;, 0.02355394098054413), (&#39;dress&#39;, 0.022519132135764744), (&#39;larger&#39;, 0.020384763234235534), (&#39;zipper&#39;, 0.019640993684217505), (&#39;too&#39;, 0.01934745458460365), (&#39;all&#39;, 0.019074915582195817)] . What we typically get with topic modelling is key words associated with each topic. In the case above, we can see key words associated with sizes: size, fit, larger. Let&#39;s take a look at some texts associated with this topic to confirm our intuition. . ex_ind = [i for i, x in enumerate(topics) if x == 1] ex_txt = [x for i, x in enumerate(texts) if i in ex_ind] for t in ex_txt[:10]: print(t) . Really cute mug. I would have given 5 stars if it were a bit bigger. Not the size I hoped for but that could be partly my fault. It did come in a very nice gift bag with the brand name on it but I just wish that it was a bead or two larger. Otherwise this is a great gift for someone with a petite wrist. Its o.k. but not as thick as another brand I previously used. I think the other brand lasted longer in my hair for the day. I wish I could give 5 stars. As far as the glasses go, I absolutely love them. But three glasses arrived completely shattered The size was off, I usually wear a lrg. or x-lrg. But this was snug I wanted to order larger but was sold out. The top was a bit tight and I&#39;m a 36 B. I got a medium. I prob would still wear top but underboob is inevitable since the straps are not adjustable. Otherwise the top was cute. Bottoms fit weird and where the strappy parts are on each side the inner lining (tan/white material) showed no matter what and looked super odd. Not cute at all. Maybe I am just too wide for them. I have a 26&#34; waist. Def for SHORT PETITE people. I really want to give this suit a 5 star but I can‚Äôt. The appearance is beautiful and I love the color. But sadly the top is to big. I followed the sizing chart for around the bust size. It all fits there but the cup size in a xxl looks as if it is a triple d or a double d. I am a larger girl being 249 but my chest is smaller. Would love to exchange sizes but cant find anywhere to message sender. I ordered a size up because my butt is larger than the rest of me, and like every other pair of jeans/shorts I buy, the waist is too big. You can see my underwear in these if I don‚Äôt have something underneath. They are good quality though. Love this dress, I probably should order a smaller size since it is a bit loose in the top and very long on me. The waist is too high and the bottom too long. I could get away with it but I like my leggings to be be fitted. I might have them altered or I send them back. Not sure yet. Fabric is on the thin size but not see through. Expected for the price. I am 5.2 so I would recommend for taller people! It adjusts well to my size which I am small/medium legging size. Perhaps they could create a petite size! . Indeed, most of these texts talk about sizes! Looks like the model is onto something! . What if we overlay the topics discovered here with our initial scatter plot? Let&#39;s try it! Now, instead of categories, we will color the dots according to the topic assigned by BERTopic algorithm. . topic_words = [&#39;-1: outlier&#39;] for i in range(len(set(topics))-1): tpc = model.get_topic(i)[:7] words = [x[0] for x in tpc] tw = &#39; &#39;.join([str(i) + &#39;:&#39;] + words) topic_words.append(tw) . exp_topics = [topic_words[x+1] for x in topics] . clrs = random.sample(Turbo256, len(set(topics))) color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs) . list_x = out[:,0] list_y = out[:,1] desc = texts source = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=exp_topics)) hover = HoverTool(tooltips=[ (&quot;index&quot;, &quot;$index&quot;), (&#39;desc&#39;, &#39;@desc&#39;), (&#39;topic&#39;, &#39;@topic&#39;) ]) p = figure(plot_width=1200, plot_height=600, tools=[hover], title=&quot;Test&quot;) p.circle(&#39;x&#39;, &#39;y&#39;, size=10, source=source, fill_color=transform(&#39;topic&#39;, color_map), # legend=&#39;topic&#39; ) # p.legend.location = &quot;top_left&quot; # p.legend.click_policy=&quot;hide&quot; bpl.show(p) . &lt;!DOCTYPE html&gt; First Look at the Data . . In this visual, the topics are clustered together - which makes sense, because the method for creating visual and topics is consistent. Interestingly, when looking at clusters of outliers that are located near each other in the chart, we can see common theme - I wonder why these were tagged as outliers? . LDA with Mallet . Let&#39;s now turn to a classic approach - LDA, Latent Dirichlet Allocation. We will not review the theory or the inner workings of this algorithm here. The key difference vs. BERTopic is that each text (document) is considered to be a composition of topics. We don&#39;t cluster documents into topics, but instead discover abstract topics that are represented in a document corpus. For each document, we get the probability distribution over these topics. . Let&#39;s imagine we have discovered three topics: sports, data science, competition. . A document that is about data science competition might have the following distribution: sports: 0.05, data science: 0.5, competition: 0.045. . A document that talks about world championship in cricket migth have the following distribution instead: sports: 0.54, data science: 0.01, competition: 0.45. . There seem to be many implementations of the LDA algorithm, and some of them result in significantly worse results. It also seems that the Mallet implementation is considered one of the best ones, so we will use it here. . To speed things up, I will use the first 10.000 reviews for topic modeling. I will only display 1000 reviews in the t-sne chart. . Imports and installation . !pip install -Uqq gensim==3.8.3 . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24.2MB 143kB/s . import os #importing os to set environment variable def install_java(): !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null #install openjdk os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; #set environment variable !java -version #check java version install_java() . openjdk version &#34;11.0.11&#34; 2021-04-20 OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04) OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing) . !wget -q http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip !unzip -qq mallet-2.0.8.zip . import gensim import gensim.corpora as corpora from gensim.utils import simple_preprocess from gensim.models.wrappers import LdaMallet from gensim.models.coherencemodel import CoherenceModel from gensim import similarities import os.path import re import glob import nltk nltk.download(&#39;stopwords&#39;) from nltk.tokenize import RegexpTokenizer from nltk.corpus import stopwords . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. . os.environ[&#39;MALLET_HOME&#39;] = &#39;/content/mallet-2.0.8&#39; mallet_path = &#39;/content/mallet-2.0.8/bin/mallet&#39; # you should NOT need to change this . def preprocess_data(doc_set,extra_stopwords = {}): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # replace all newlines or multiple sequences of spaces with a standard space doc_set = [re.sub(&#39; s+&#39;, &#39; &#39;, doc) for doc in doc_set] # initialize regex tokenizer tokenizer = RegexpTokenizer(r&#39; w+&#39;) # create English stop words list en_stop = set(stopwords.words(&#39;english&#39;)) # add any extra stopwords if (len(extra_stopwords) &gt; 0): en_stop = en_stop.union(extra_stopwords) # list for tokenized documents in loop texts = [] # loop through document list for i in doc_set: # clean and tokenize document string raw = i.lower() tokens = tokenizer.tokenize(raw) # remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop] # add tokens to list texts.append(stopped_tokens) return texts def prepare_corpus(doc_clean): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean) dictionary = corpora.Dictionary(doc_clean) dictionary.filter_extremes(no_below=5, no_above=0.5) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # generate LDA model return dictionary,doc_term_matrix . Topic modelling with LDA . LDA requires some careful parameter choices to work properly. These seem to be expecially relevant: . number of topics | stop words list | alpha parameter, which roughly determines how many topics correspond to a single document | . # texts_lda = [dataset[&#39;train&#39;][i][&#39;review_body&#39;] for i in range(10000)] . doc_clean = preprocess_data(texts,{}) dictionary, doc_term_matrix = prepare_corpus(doc_clean) . number_of_topics=30 # adjust this to alter the number of topics words=10 #adjust this to alter the number of words output for the topic below . ldamallet = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dictionary, alpha=10) . topic_words = ldamallet.show_topics(num_topics=number_of_topics,num_words=5) topic_words = [x[1] for x in topic_words] . topic_words = [] for i in range(number_of_topics): tpc = ldamallet.show_topic(i, topn=7, num_words=None) words = [x[0] for x in tpc] tw = &#39; &#39;.join([str(i) + &#39;:&#39;] + words) topic_words.append(tw) . topic_words . [&#39;0: case small love feels design bit camera&#39;, &#39;1: perfect started fall heavy weight quickly feet&#39;, &#39;2: nice day box gift looked purchased shoe&#39;, &#39;3: 2 3 5 stars 1 4 weeks&#39;, &#39;4: work bought make fine cut pump job&#39;, &#39;5: broke side soft beautiful ring long bottom&#39;, &#39;6: bag product picture package show guess happy&#39;, &#39;7: hard money working lot worth worked things&#39;, &#39;8: color light colors white loves lights daughter&#39;, &#39;9: water plastic open air hold inside difficult&#39;, &#39;10: size fit wear ordered order comfortable big&#39;, &#39;11: put easy bought left times piece face&#39;, &#39;12: arrived nice pieces broken returned completely thin&#39;, &#39;13: quality made work easily poor fits low&#39;, &#39;14: book love great pages missing family star&#39;, &#39;15: great purchase cover screen purchased recommended replace&#39;, &#39;16: great works recommend lots price smells awesome&#39;, &#39;17: product bad month disappointed reason sound needed&#39;, &#39;18: buy review year frame support difficult idea&#39;, &#39;19: good fit bit brand fine watch screws&#39;, &#39;20: top set problem short expected people story&#39;, &#39;21: item return back shipping received disappointed send&#39;, &#39;22: easy recommend install works clean thick 10&#39;, &#39;23: fast battery charge wrong 4 year cord&#39;, &#39;24: back extra front chair makes pull returning&#39;, &#39;25: phone thing years home stay friend find&#39;, &#39;26: received hair order ordered amazon seller problems&#39;, &#39;27: time loved cute super long huge toy&#39;, &#39;28: good price quality pretty decent expect end&#39;, &#39;29: cheap material perfect loose buy big 5&#39;] . topics_docs = list() for m in ldamallet[doc_term_matrix[:1000]]: topics_docs.append(m) . x = np.array(topics_docs[:1000]) y = np.delete(x,0,axis=2) y = y.squeeze() . best_topics = np.argmax(y, axis=1) topics = list(best_topics) topics = [topic_words[x] for x in topics] . # up to 20 colors: # palette = d3[&#39;Category20&#39;][number_of_topics] clrs = random.sample(Turbo256, number_of_topics) color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs) . list_x = out[:,0] list_y = out[:,1] desc = texts source = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=topics)) hover = HoverTool(tooltips=[ (&quot;index&quot;, &quot;$index&quot;), (&#39;desc&#39;, &#39;@desc&#39;), (&#39;topic&#39;, &#39;@topic&#39;) ]) p = figure(plot_width=1200, plot_height=600, tools=[hover], title=&quot;Test&quot;) p.circle(&#39;x&#39;, &#39;y&#39;, size=10, source=source, fill_color=transform(&#39;topic&#39;, color_map), # legend=&#39;topic&#39; ) # p.legend.location = &quot;top_left&quot; # p.legend.click_policy=&quot;hide&quot; bpl.show(p) . &lt;!DOCTYPE html&gt; First Look at the Data . . Looking at the chart above, it seems that the topics identified by LDA re not necessarily close to each other on the chart. We know that the chart should align well with topics identified by SentenceBert embeddings, so this confirms that both approaches are complementary and can result in a different set of topics. . My intuition is that BERTopic should work better for shorter documents, that are more likely to represent a single topic, while LDA may handle better longer texts that are likely a combination of topics. Both can offer some good insights into our documents, so it makes sense to try both! . Let&#39;s also look at the LDA topics visualizations through the PyLDAVis library. . PyLDAVis charts . gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet) . !pip install -Uqq pyLDAvis==2.1.2 . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6MB 6.7MB/s Building wheel for pyLDAvis (setup.py) ... done . import pyLDAvis import pyLDAvis.gensim . /usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated since Python 3.3,and in 3.9 it will stop working from collections import Iterable /usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated since Python 3.3,and in 3.9 it will stop working from collections import Mapping . pyLDAvis.enable_notebook() p = pyLDAvis.gensim.prepare(gensimmodel, doc_term_matrix, dictionary) p . /usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T)) /usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:240: RuntimeWarning: divide by zero encountered in log log_lift = np.log(topic_term_dists / term_proportion) /usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:241: RuntimeWarning: divide by zero encountered in log log_ttd = np.log(topic_term_dists) . Further Research . Below you can find links to some additional materials, research and tools related to topic modeling. . Top2Vec . https://github.com/ddangelov/Top2Vec | . Topic modeling in embedding spaces . https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00325 | . The Dynamic Embedded Topic Model . evolution of topics over time | https://arxiv.org/pdf/1907.05545.pdf | . TopicScan (NMF) . https://github.com/derekgreene/topicscan | . Topic model evaluation . https://www.aclweb.org/anthology/E14-1056.pdf | . Improving Neural Topic Models using Knowledge Distillation . https://www.aclweb.org/anthology/2020.emnlp-main.137.pdf | https://twitter.com/miserlis_/status/1305893876767612929 | https://github.com/ahoho/kd-topic-models | . Neural Topic Modeling by Incorporating Document Relationship Graph . Applying Graph NN to topic modeling | https://www.aclweb.org/anthology/2020.emnlp-main.310.pdf | . Weakly-Supervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding . unsupervised combination of topic modeling and and aspect-based sentiment tagging | https://github.com/teapot123/JASen | https://arxiv.org/pdf/2010.06705v1.pdf | . Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too! . Similar to top2vec? | https://arxiv.org/pdf/2004.14914v2.pdf | https://github.com/adalmia96/Cluster-Analysis | . Cross-lingual Contextualized Topic Models with Zero-shot Learning . model learns topics in one language and predicts them for documents in another language | https://arxiv.org/pdf/2004.07737v1.pdf | . Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence . Another variation on neural topic modeling | https://arxiv.org/pdf/2004.03974v1.pdf | . Familia: A Configurable Topic Modeling Framework for Industrial Text Engineering . Baidu‚Äôs framework for topic modeling and overview of use cases | https://arxiv.org/pdf/1808.03733v2.pdf | . Aspect Sentiment Model for Micro Reviews . https://arxiv.org/pdf/1806.05499v1.pdf | . Studying the History of Ideas Using Topic Models . nice application of topic models by jurafsky, manning - get inspiration | https://web.stanford.edu/~jurafsky/hallemnlp08.pdf | . CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring . User inputs a partial taxonomy, and CoRel extracts a more complete topical taxonomy based on user-interested aspect and relation, with each node represented by a cluster of words. | https://arxiv.org/pdf/2010.06714.pdf | .",
            "url": "https://skok.ai/2021/05/27/Topic-Models-Introduction.html",
            "relUrl": "/2021/05/27/Topic-Models-Introduction.html",
            "date": " ‚Ä¢ May 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Training BERT for Cyberbullying Detection - Towards SOTA",
            "content": "This is a follow up to the previous notebook training a binary classification model that should detect cyberbullying in Polish Tweets. The dataset comes from a Polish NLP competition - PolEval 2019 (http://2019.poleval.pl/index.php/tasks/task6). It is also included in Polish NLP Benchmark KLEJ (https://klejbenchmark.com/). Our goal is to reach state-of-the-art results, with the following points of reference: . Best result in last year competition: 58.58 f1 (n-waves ULMFiT) | Best result for a base BERT model on KLEJ: 66.7 (Polish Roberta base) | Best result for a large BERT model on KLEJ: 72.4 (XLM-RoBERTa large + NKJP) | . To achieve that, we will work with the HuggingFace library and Pytorch. . Setup . Let&#39;s start by installing transformers, and importing the relevant libraries. We will now work mostly with Pytorch. . !pip install transformers -q . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 2.7MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 16.2MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 18.5MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 53.0MB/s Building wheel for sacremoses (setup.py) ... done . import pandas as pd import numpy as np import torch import torch.nn as nn from sklearn import model_selection, metrics from sklearn.metrics import accuracy_score, precision_recall_fscore_support from transformers import AdamW, get_linear_schedule_with_warmup, BertTokenizerFast, BertPreTrainedModel, BertModel, BertConfig from tqdm.autonotebook import tqdm from torch.utils.data.sampler import WeightedRandomSampler . Data preparation . Let&#39;s start with downloading the dataset and converting it into a dataframe. . !wget -q https://klejbenchmark.com/static/data/klej_cbd.zip !unzip -q klej_cbd.zip . df = pd.read_csv(&#39;train.tsv&#39;, delimiter=&#39; t&#39;) df.columns = [&#39;text&#39;, &#39;label&#39;] df.label = df.label.astype(int) df = df.dropna().reset_index(drop=True) . We will now switch from a single train-validation split into cross-validation (5 fold). We will be also more careful with the split, applying stratified k-fold split, so that each fold has similar amount of positive labels. With cross-validation, our goal is to benefit from all training data. At the same time, by ensembling the models trained on each fold, we should be reducing random errors, making our ensemble more predictable. . df[&quot;kfold&quot;] = -1 df = df.sample(frac=1, random_state=42).reset_index(drop=True) kf = model_selection.StratifiedKFold(n_splits=5) for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.label.values)): df.loc[val_, &#39;kfold&#39;] = fold df.to_csv(&#39;train.csv&#39;, index=False) . We will also apply some pre-processing of the tweets. First, we will replace &#39;@anonymized_account&#39; with &#39;@ u≈ºytkownik&#39;. Second, we will replace the emoji characters with their plain text counterparts. Both modifications are based on the Polish Roberta training scripts (https://github.com/sdadas/polish-roberta). These changes should allow the Polish BERT model better represent the text. . emoji = { &#39;üòÄ&#39;: &#39;:D&#39;, &#39;üòÉ&#39;: &#39;:)&#39;, &#39;üòÑ&#39;: &#39;:)&#39;, &#39;üòÅ&#39;: &#39;:)&#39;, &#39;üòÜ&#39;: &#39;xD&#39;, &#39;üòÖ&#39;: &#39;:)&#39;, &#39;ü§£&#39;: &#39;xD&#39;, &#39;üòÇ&#39;: &#39;xD&#39;, &#39;üôÇ&#39;: &#39;:)&#39;, &#39;üôÉ&#39;: &#39;:)&#39;, &#39;üòâ&#39;: &#39;;)&#39;, &#39;üòä&#39;: &#39;:)&#39;, &#39;üòá&#39;: &#39;:)&#39;, &#39;ü•∞&#39;: &#39;:*&#39;, &#39;üòç&#39;: &#39;:*&#39;, &#39;ü§©&#39;: &#39;:*&#39;, &#39;üòò&#39;: &#39;:*&#39;, &#39;üòó&#39;: &#39;:*&#39;, &#39;‚ò∫&#39;: &#39;:)&#39;, &#39;üòö&#39;: &#39;:*&#39;, &#39;üòã&#39;: &#39;:P&#39;, &#39;üòõ&#39;: &#39;:P&#39;, &#39;üòú&#39;: &#39;:P&#39;, &#39;üòù&#39;: &#39;:P&#39;, &#39;ü§ë&#39;: &#39;:P&#39;, &#39;ü§™&#39;: &#39;:P&#39;, &#39;ü§ó&#39;: &#39;:P&#39;, &#39;ü§≠&#39;: &#39;:P&#39;, &#39;ü§´&#39;: &#39;:|&#39;, &#39;ü§î&#39;: &#39;:|&#39;, &#39;ü§®&#39;: &#39;:|&#39;, &#39;üòê&#39;: &#39;:|&#39;, &#39;üòë&#39;: &#39;:|&#39;, &#39;üò∂&#39;: &#39;:|&#39;, &#39;üòè&#39;: &#39;:)&#39;, &#39;üòí&#39;: &#39;:(&#39;, &#39;üôÑ&#39;: &#39;:|&#39;, &#39;ü§ê&#39;: &#39;:|&#39;, &#39;üò¨&#39;: &#39;:$&#39;, &#39;üòå&#39;: &#39;zzz&#39;, &#39;üòî&#39;: &#39;:(&#39;, &#39;üò™&#39;: &#39;zzz&#39;, &#39;ü§§&#39;: &#39;:(&#39;, &#39;ü§í&#39;: &#39;:(&#39;, &#39;ü§ï&#39;: &#39;:(&#39;, &#39;ü§¢&#39;: &#39;:(&#39;, &#39;ü§Æ&#39;: &#39;:(&#39;, &#39;ü§ß&#39;: &#39;:(&#39;, &#39;ü•µ&#39;: &#39;:(&#39;, &#39;ü•∂&#39;: &#39;:(&#39;, &#39;ü•¥&#39;: &#39;:(&#39;, &#39;üòµ&#39;: &#39;:(&#39;, &#39;ü§Ø&#39;: &#39;:(&#39;, &#39;ü§†&#39;: &#39;:)&#39;, &#39;ü•≥&#39;: &#39;:)&#39;, &#39;üòé&#39;: &#39;:)&#39;, &#39;ü§ì&#39;: &#39;:)&#39;, &#39;üßê&#39;: &#39;:)&#39;, &#39;üòï&#39;: &#39;:(&#39;, &#39;üòü&#39;: &#39;:(&#39;, &#39;üôÅ&#39;: &#39;:(&#39;, &#39;‚òπ&#39;: &#39;:(&#39;, &#39;üòÆ&#39;: &#39;:O&#39;, &#39;üòØ&#39;: &#39;:O&#39;, &#39;üò≤&#39;: &#39;:O&#39;, &#39;üò≥&#39;: &#39;:(&#39;, &#39;ü•∫&#39;: &#39;:(&#39;, &#39;üò¶&#39;: &#39;:(&#39;, &#39;üòß&#39;: &#39;:(&#39;, &#39;üò®&#39;: &#39;:(&#39;, &#39;üò∞&#39;: &#39;:(&#39;, &#39;üò•&#39;: &#39;:(&#39;, &#39;üò¢&#39;: &#39;:(&#39;, &#39;üò≠&#39;: &#39;:(&#39;, &#39;üò±&#39;: &#39;:(&#39;, &#39;üòñ&#39;: &#39;:(&#39;, &#39;üò£&#39;: &#39;:(&#39;, &#39;üòû&#39;: &#39;:(&#39;, &#39;üòì&#39;: &#39;:(&#39;, &#39;üò©&#39;: &#39;:(&#39;, &#39;üò´&#39;: &#39;:(&#39;, &#39;ü•±&#39;: &#39;zzz&#39;, &#39;üò§&#39;: &#39;:(&#39;, &#39;üò°&#39;: &#39;:(&#39;, &#39;üò†&#39;: &#39;:(&#39;, &#39;ü§¨&#39;: &#39;:(&#39;, &#39;üòà&#39;: &#39;]:-&gt;&#39;, &#39;üëø&#39;: &#39;]:-&gt;&#39;, &#39;üíÄ&#39;: &#39;:(&#39;, &#39;‚ò†&#39;: &#39;:(&#39;, &#39;üíã&#39;: &#39;:*&#39;, &#39;üíî&#39;: &#39;:(&#39;, &#39;üí§&#39;: &#39;zzz&#39; } . df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda r: r.replace(&quot;@anonymized_account&quot;, &quot;@ u≈ºytkownik&quot;)) . df[&#39;text&#39;] = df[&#39;text&#39;].apply(lambda r: &quot;&quot;.join((emoji.get(c, c) for c in r))) . Helper functions . class AverageMeter: &quot;&quot;&quot; Computes and stores the average and current value &quot;&quot;&quot; def __init__(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n=1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count . Configuration . Let&#39;s define some key hyperparameters that influence our training: . max length: how many tokens should be used per tweet? Based on the training data, the longest tweet is 91 tokens with Polbert tokenizer, so we will set the max length to 92 tokens and pad all tokens to that length with [PAD] token. | batch size: we will use batch size 64, it might be difficult to use a bigger one on some GPUs | number of epochs: our dataset is fairly small, so training for a large number of epochs might lead to overfitting. Let&#39;s set on 2 epochs here. | learning rate: we will use discriminative learning rate, applying a higher learning rate to the classifier layer (that we start with random weights), and a lower learning rate to the encoder (which has been pretrained so it already should have &#39;good&#39; weights) | warm up: we will be using linear schedule with warm up, so the learning rate will be increased for the number of steps defined here, and then linearly decreased to zero | pretrained model and tokenizer: we will work again with Polbert uncased model | . MAX_LEN = 92 TRAIN_BATCH_SIZE = 64 VALID_BATCH_SIZE = 64 EPOCHS = 2 LR = 2e-5 HEAD_LR = 1e-4 WARMUP_STEPS = 30 BERT_PATH = &#39;dkleczek/bert-base-polish-uncased-v1&#39; TOKENIZER = BertTokenizerFast.from_pretrained(&#39;dkleczek/bert-base-polish-uncased-v1&#39;) . Pytorch Dataset and Model . Let&#39;s start by defining Pytorch Dataset. It needs to implement the len and getitem methods. We will again use the HuggingFace tokenizer to convert text into input_ids, mask and token_type_ids that are expected by our BERT layer. . class CBDDataset: def __init__(self, text, label): self.text = text self.label = label self.tokenizer = TOKENIZER def __len__(self): return len(self.text) def __getitem__(self, item): text = &#39; &#39;.join(self.text[item].split()) label = self.label[item] enc = self.tokenizer(text, max_length=MAX_LEN, truncation=True, padding=&#39;max_length&#39;, return_tensors=&#39;pt&#39;) return { &#39;ids&#39;: enc.input_ids[0], &#39;mask&#39;: enc.attention_mask[0], &#39;token_type_ids&#39;: enc.token_type_ids[0], &#39;targets&#39;: torch.tensor(label, dtype=torch.long) } . Now is the time to define our model! First, let&#39;s look at the elements that are normally expected: . bert layer: the entire BERT pretrained model is a single layer in our model. We are using again pretrained weights from HuggingFace hub. | drop out: it&#39;s another hyperparameter that can be tuned, here we set it directly in the model | linear classification layer: this is a binary classification problem with 2 classes (True and False) and we define a linear layer for this. This comes with random weights that we initialize here. | . We are also doing some modifications here that should help us improve the results: . using the full hidden state rather than [CLS] token output: there is some research showing that the last layers of pretrained model are very specific to pretraining task and don&#39;t help in finetuning. We will output all hidden states from the model and use the penultimate layer (-2) for our task | max pooling: we will take the output from all tokens (768 features * 92 tokens) and take the max value for each feature across all tokens. The intutition here is that the model may encode &#39;cyberbullying&#39; in the token representation, and if it&#39;s contained somewhere in a tweet, we should use that information. | . class CBDModel(BertPreTrainedModel): def __init__(self, conf): super(CBDModel, self).__init__(conf) self.bert = BertModel.from_pretrained(BERT_PATH, config=conf) self.mx = nn.MaxPool1d(MAX_LEN) self.drop_out = nn.Dropout(0.5) self.l0 = nn.Linear(768, 2) torch.nn.init.normal_(self.l0.weight, std=0.02) def forward(self, ids, mask, token_type_ids): _, _, out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids) out = out[-2] out = out.permute(0,2,1) out = torch.squeeze(self.mx(out)) out = self.drop_out(out) out = self.l0(out) return out . We will use cross entropy loss here. . def loss_fn(outputs, targets): return nn.CrossEntropyLoss()(outputs, targets) . Training and Evaluation Loop with Weighted Random Sampling . In this section, we define our training and evaluation functions and the runner that executes the training. The key modification here is using Weigthed Random Sampler to address the class imbalance issue. . def train_fn(data_loader, model, optimizer, device, scheduler=None): model.train() losses = AverageMeter() f1s = AverageMeter() tk0 = tqdm(data_loader, total=len(data_loader)) for bi, d in enumerate(tk0): ids = d[&quot;ids&quot;] token_type_ids = d[&quot;token_type_ids&quot;] mask = d[&quot;mask&quot;] targets = d[&quot;targets&quot;] ids = ids.to(device, dtype=torch.long) token_type_ids = token_type_ids.to(device, dtype=torch.long) mask = mask.to(device, dtype=torch.long) targets = targets.to(device, dtype=torch.long) model.zero_grad() outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids) loss = loss_fn(outputs, targets) loss.backward() optimizer.step() scheduler.step() outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy() targets = targets.cpu().detach().numpy().astype(int) f1 = metrics.f1_score(targets,outputs) f1s.update(f1, ids.size(0)) losses.update(loss.item(), ids.size(0)) tk0.set_postfix(loss=losses.avg, f1=f1s.avg) . def eval_fn(data_loader, model, device): model.eval() fin_targets = [] fin_outputs = [] with torch.no_grad(): for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)): ids = d[&quot;ids&quot;] token_type_ids = d[&quot;token_type_ids&quot;] mask = d[&quot;mask&quot;] targets = d[&quot;targets&quot;] ids = ids.to(device, dtype=torch.long) token_type_ids = token_type_ids.to(device, dtype=torch.long) mask = mask.to(device, dtype=torch.long) outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist() fin_targets.extend(targets.cpu().detach().numpy().tolist()) fin_outputs.extend(outputs) f1 = metrics.f1_score(fin_targets,fin_outputs) return f1 . def run(fold): df_train = df[df.kfold != fold].reset_index(drop=True) df_valid = df[df.kfold == fold].reset_index(drop=True) # df_train = df_train[:64] # df_valid = df_valid[:64] target = df_train.label.values class_sample_count = np.array([len(np.where(target == t)[0]) for t in np.unique(target)]) weight = 1. / class_sample_count samples_weight = np.array([weight[t] for t in target]) samples_weight = torch.from_numpy(samples_weight) samples_weigth = samples_weight.double() sampler = WeightedRandomSampler(samples_weight, len(samples_weight)) train_dataset = CBDDataset( text=df_train.text.values, label=df_train.label.values, ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=1, sampler=sampler ) valid_dataset = CBDDataset( text=df_valid.text.values, label=df_valid.label.values, ) valid_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=2 ) device = torch.device(&quot;cuda&quot;) model_config = BertConfig.from_pretrained(BERT_PATH) model_config.output_hidden_states = True model = CBDModel(conf=model_config) model.to(device) num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS) param_optimizer = list(model.named_parameters())[:-2] no_decay = [&quot;bias&quot;, &quot;LayerNorm.bias&quot;, &quot;LayerNorm.weight&quot;] optimizer_parameters = [ {&#39;params&#39;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.01}, {&#39;params&#39;: [p for n, p in param_optimizer if (any(nd in n for nd in no_decay))], &#39;weight_decay&#39;: 0.0}, {&#39;params&#39;: model.l0.weight, &quot;lr&quot;: HEAD_LR, &#39;weight_decay&#39;: 0.01}, {&#39;params&#39;: model.l0.bias, &quot;lr&quot;: HEAD_LR, &#39;weight_decay&#39;: 0.0}, ] optimizer = AdamW(optimizer_parameters, lr=LR) scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=num_train_steps ) print(f&quot;Training is starting for fold: {fold}&quot;) for epoch in range(EPOCHS): train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler) f1 = eval_fn(valid_data_loader, model, device) print(f&quot;Epoch: {epoch}, F1 score = {f1}&quot;) model_path=f&quot;model_{fold}.bin&quot; torch.save(model.state_dict(), model_path) . Let&#39;s train! . run(0) . Training is starting for fold: 0 Epoch: 0, F1 score = 0.4273255813953489 Epoch: 1, F1 score = 0.5588235294117647 . run(1) . Training is starting for fold: 1 Epoch: 0, F1 score = 0.4831460674157303 Epoch: 1, F1 score = 0.5245901639344263 . run(2) . Training is starting for fold: 2 Epoch: 0, F1 score = 0.4551083591331269 Epoch: 1, F1 score = 0.5592233009708738 . run(3) . Training is starting for fold: 3 Epoch: 0, F1 score = 0.4318181818181818 Epoch: 1, F1 score = 0.5283018867924528 . run(4) . Training is starting for fold: 4 Epoch: 0, F1 score = 0.47330960854092524 Epoch: 1, F1 score = 0.5536480686695279 . Evaluation and results . We have now trained 5 models on different folds. Let&#39;s apply these models on our test set, pre-processed in the same way as our training set. We will average the raw logits (outputs) from each model, and then apply argmax to choose the outputted class. . !wget -q https://raw.githubusercontent.com/ptaszynski/cyberbullying-Polish/master/task%2001/test_set_clean_only_tags.txt . df_test = pd.read_csv(&#39;test_features.tsv&#39;, delimiter=&#39; t&#39;) df_test.columns = [&#39;text&#39;] df_test[&#39;text&#39;] = df_test[&#39;text&#39;].apply(lambda r: r.replace(&quot;@anonymized_account&quot;, &quot;@ u≈ºytkownik&quot;)) df_test[&#39;text&#39;] = df_test[&#39;text&#39;].apply(lambda r: &quot;&quot;.join((emoji.get(c, c) for c in r))) df_test[&#39;label&#39;] = 0 df_lbls = pd.read_csv(&#39;test_set_clean_only_tags.txt&#39;,names=[&#39;label&#39;]) labels = df_lbls.label.values . device = torch.device(&quot;cuda&quot;) model_config = BertConfig.from_pretrained(BERT_PATH) model_config.output_hidden_states = True . model1 = CBDModel(conf=model_config) model1.to(device) model1.load_state_dict(torch.load(&quot;model_0.bin&quot;)) model1.eval() model2 = CBDModel(conf=model_config) model2.to(device) model2.load_state_dict(torch.load(&quot;model_1.bin&quot;)) model2.eval() model3 = CBDModel(conf=model_config) model3.to(device) model3.load_state_dict(torch.load(&quot;model_2.bin&quot;)) model3.eval() model4 = CBDModel(conf=model_config) model4.to(device) model4.load_state_dict(torch.load(&quot;model_3.bin&quot;)) model4.eval() model5 = CBDModel(conf=model_config) model5.to(device) model5.load_state_dict(torch.load(&quot;model_4.bin&quot;)) model5.eval(); . final_output = [] test_dataset = CBDDataset( text=df_test.text.values, label=df_test.label.values, ) data_loader = torch.utils.data.DataLoader( test_dataset, shuffle=False, batch_size=VALID_BATCH_SIZE, num_workers=1 ) with torch.no_grad(): tk0 = tqdm(data_loader, total=len(data_loader)) for bi, d in enumerate(tk0): ids = d[&quot;ids&quot;] token_type_ids = d[&quot;token_type_ids&quot;] mask = d[&quot;mask&quot;] ids = ids.to(device, dtype=torch.long) token_type_ids = token_type_ids.to(device, dtype=torch.long) mask = mask.to(device, dtype=torch.long) outputs1 = model1(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs2 = model2(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs3 = model3(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs4 = model4(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs5 = model5(ids=ids, mask=mask, token_type_ids=token_type_ids) outputs = (outputs1 + outputs2 + outputs3 + outputs4 + outputs5) / 5 outputs = torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist() final_output.extend(outputs) . . precision, recall, f1, _ = precision_recall_fscore_support(labels, final_output, average=&#39;binary&#39;) acc = accuracy_score(labels, final_output) print( { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall }) . {&#39;accuracy&#39;: 0.905, &#39;f1&#39;: 0.671280276816609, &#39;precision&#39;: 0.6258064516129033, &#39;recall&#39;: 0.7238805970149254} . This looks good! Our F1 score is around 0.66 - 0.68, which is in the range of the best base BERT model on KLEJ Benchmark (Polish Roberta base reported results in the range 0.63-0.69). . Improvements . What can be done to further improve the results? Here are some ideas: . Data augmentation. Can we add more variety/examples via text augmentation? | More hyperparameter tuning. Key watch out is to ensure a good cross-validation approach, so that we don&#39;t tune on the test set. | Multi-sample dropout. This technique was used by winning teams in recent Kaggle NLP competitions. | Multi-lingual transfer. We have large toxicity datasets in English, can we use that with a multi-lingual model like XLM-Roberta to classify Polish Tweets? | Multi-task learning. We could benefit from training a single model on several tasks, e.g. from KLEJ Benchmark, to see if that helps. | Ensembling/Stacking. Ensembling results across models with different encoders and fine-tuning protocols is very likely to improve the score even further. | .",
            "url": "https://skok.ai/2020/10/27/Training-BERT-for-Cyberbullying-Detection-Part-2.html",
            "relUrl": "/2020/10/27/Training-BERT-for-Cyberbullying-Detection-Part-2.html",
            "date": " ‚Ä¢ Oct 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Training BERT for Cyberbullying Detection - HF Trainer Baseline",
            "content": "Our goal is to train a binary classification model that should detect cyberbullying in Polish Tweets. The dataset comes from a Polish NLP competition - PolEval 2019 (http://2019.poleval.pl/index.php/tasks/task6). It is also included in Polish NLP Benchmark KLEJ (https://klejbenchmark.com/). . Setup . Let&#39;s start by installing two libraries from HuggingFace that will make our job easier: transformers and datasets. We will also import the relevant libraries. . !pip install transformers -qq !pip install datasets -qq . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.3MB 3.4MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 890kB 19.2MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1MB 29.8MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 43.7MB/s Building wheel for sacremoses (setup.py) ... done |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 153kB 3.3MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17.7MB 204kB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245kB 64.2MB/s . import numpy as np import pandas as pd import torch from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments from datasets import load_dataset, Dataset from sklearn.metrics import accuracy_score, precision_recall_fscore_support . In this demo, we will use the Polish pretrained BERT model - Polbert (https://github.com/kldarek/polbert). It can be downloaded from the HuggingFace model hub, and we will use BertForSequenceClassification class to load it. We will also need the Polbert tokenizer. . model = BertForSequenceClassification.from_pretrained(&#39;dkleczek/bert-base-polish-uncased-v1&#39;) tokenizer = BertTokenizerFast.from_pretrained(&#39;dkleczek/bert-base-polish-uncased-v1&#39;) . . Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dkleczek/bert-base-polish-uncased-v1 and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . . Now we will load the training data from the KLEJ benchmark website, clean it up, and convert to a csv format that can be used to create a dataset. . Data . !wget -q https://klejbenchmark.com/static/data/klej_cbd.zip !unzip -q klej_cbd.zip . --2020-10-26 08:29:07-- https://klejbenchmark.com/static/data/klej_cbd.zip Resolving klejbenchmark.com (klejbenchmark.com)... 35.234.99.58 Connecting to klejbenchmark.com (klejbenchmark.com)|35.234.99.58|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 375476 (367K) [application/zip] Saving to: ‚Äòklej_cbd.zip‚Äô klej_cbd.zip 100%[===================&gt;] 366.68K 866KB/s in 0.4s 2020-10-26 08:29:08 (866 KB/s) - ‚Äòklej_cbd.zip‚Äô saved [375476/375476] Archive: klej_cbd.zip inflating: test_features.tsv inflating: train.tsv . df = pd.read_csv(&#39;train.tsv&#39;, delimiter=&#39; t&#39;) df = df.dropna().reset_index(drop=True) df.columns = [&#39;text&#39;, &#39;label&#39;] df.label = df.label.astype(int) df = df.sample(frac=1, random_state=42) df.to_csv(&#39;train.csv&#39;, index=False) len(df), len(df[df.label == 1]) . (10041, 851) . Our training set consists of 10 thousand tweets, but only 851 of those tweets are tagged as cyberbullying. We can take a look at a sample of data in the dataframe. . df.head() . text label . 5809 LUDZIE Z BYDGOSZCZY: NAJLEPSZA RESTAURACJA? Rt... | 0 | . 5938 @anonymized_account Sta≈Çam na zewnƒÖtrz, ale ma... | 0 | . 2260 RT @anonymized_account Halicki: proszƒô nie m√≥w... | 0 | . 8833 @anonymized_account @anonymized_account Czyli ... | 1 | . 4513 @anonymized_account Ju≈º nic nie bƒôdzie takie s... | 0 | . Dataset . We need to convert our data into a format that can be fed to our model. Thanks to HuggingFace datesets library magic, we con do this with just a few lines of code. We will load the dataset from csv file, split it into train (80%) and validation set (20%). We will then map the tokenizer to convert the text strings into a format that can be fed into BERT model (input_ids and attention mask). Finally, we&#39;ll convert that into torch tensors. . train_dataset, test_dataset = load_dataset(&#39;csv&#39;, data_files=&#39;train.csv&#39;, split=[&#39;train[:80%]&#39;, &#39;train[80%:]&#39;]) . Using custom data configuration default . Downloading and preparing dataset csv/default-013faa159f500b12 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-013faa159f500b12/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4... Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-013faa159f500b12/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4. Subsequent calls will reuse this data. . # train_dataset[0] . def tokenize(batch): return tokenizer(batch[&#39;text&#39;], padding=True, truncation=False) train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset)) test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(train_dataset)) . . # train_dataset[0] . train_dataset.set_format(&#39;torch&#39;, columns=[&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;]) test_dataset.set_format(&#39;torch&#39;, columns=[&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;]) . # train_dataset[0] . Training and Evaluation . We are almost ready to start the training. Let&#39;s define a function that will help us monitor the training progress and evaluate results on the validation dataset. We will primarily focus on F1, recall and precision metrics, especially that F1 is the official evaluation metric for this dataset. . def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#39;binary&#39;) acc = accuracy_score(labels, preds) return { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall } . HuggingFace wraps up the default transformer fine-tuning approach in the Trainer object, and we can customize it by passing training arguments such as learning rate, number of epochs, batch size etc. We will set logging_steps to 20, so that we can frequently evaluate how the model performs on the validation set throughout the training. . training_args = TrainingArguments( output_dir=&#39;./results&#39;, learning_rate=2e-5, num_train_epochs=3, per_device_train_batch_size=64, per_device_eval_batch_size=64, fp16=True, warmup_steps=30, logging_steps=20, weight_decay=0.01, evaluate_during_training=True, logging_dir=&#39;./logs&#39;, ) trainer = Trainer( model=model, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=test_dataset ) . /usr/local/lib/python3.6/dist-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options) FutureWarning, . trainer.train() . /usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:847: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.) return torch.tensor(x, **format_kwargs) . . [378/378 05:31, Epoch 3/3] Step Training Loss Validation Loss Accuracy F1 Precision Recall . 20 | 0.581389 | 0.320318 | 0.907869 | 0.000000 | 0.000000 | 0.000000 | . 40 | 0.268629 | 0.245489 | 0.908367 | 0.010753 | 1.000000 | 0.005405 | . 60 | 0.216368 | 0.229870 | 0.914841 | 0.149254 | 0.937500 | 0.081081 | . 80 | 0.207710 | 0.255416 | 0.913845 | 0.121827 | 1.000000 | 0.064865 | . 100 | 0.187619 | 0.178497 | 0.926793 | 0.374468 | 0.880000 | 0.237838 | . 120 | 0.207315 | 0.169160 | 0.931275 | 0.589286 | 0.655629 | 0.535135 | . 140 | 0.155866 | 0.170657 | 0.929283 | 0.564417 | 0.652482 | 0.497297 | . 160 | 0.173842 | 0.173169 | 0.930279 | 0.554140 | 0.674419 | 0.470270 | . 180 | 0.128828 | 0.174443 | 0.927291 | 0.522876 | 0.661157 | 0.432432 | . 200 | 0.144621 | 0.169735 | 0.930777 | 0.535117 | 0.701754 | 0.432432 | . 220 | 0.122884 | 0.167735 | 0.929781 | 0.568807 | 0.654930 | 0.502703 | . 240 | 0.134178 | 0.168437 | 0.927291 | 0.535032 | 0.651163 | 0.454054 | . 260 | 0.094126 | 0.169739 | 0.932769 | 0.571429 | 0.692308 | 0.486486 | . 280 | 0.079876 | 0.183835 | 0.931275 | 0.574074 | 0.669065 | 0.502703 | . 300 | 0.091462 | 0.203578 | 0.930279 | 0.545455 | 0.682927 | 0.454054 | . 320 | 0.074335 | 0.195306 | 0.930777 | 0.582583 | 0.655405 | 0.524324 | . 340 | 0.079284 | 0.202131 | 0.932769 | 0.563107 | 0.701613 | 0.470270 | . 360 | 0.082395 | 0.193977 | 0.931275 | 0.584337 | 0.659864 | 0.524324 | . &lt;/div&gt; &lt;/div&gt; /usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior. _warn_prf(average, modifier, msg_start, len(result)) . TrainOutput(global_step=378, training_loss=0.16361538316837695) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We are reaching around 0.58 - 0.62 F1 score on the validation set (the actual result will vary since we&#39;re not fixing the seeds here). We can also see the training progress in the tensorboard charts that read our training log. . trainer.evaluate() . . [32/32 00:04] {&#39;epoch&#39;: 3.0, &#39;eval_accuracy&#39;: 0.9327689243027888, &#39;eval_f1&#39;: 0.5970149253731343, &#39;eval_loss&#39;: 0.19227837026119232, &#39;eval_precision&#39;: 0.6666666666666666, &#39;eval_recall&#39;: 0.5405405405405406, &#39;total_flos&#39;: 1738480015991628} . %load_ext tensorboard %tensorboard --logdir logs . Result Evaluation . Given that this is a completed competition, we have access to the test set. We shouldn&#39;t be using it for validation, to avoid presenting overfitted results, but we can use it to see how our solution ranks against the benchmarks. Let&#39;s download that data and evaluate the model on it. We will need to repeat some of the steps we applied on the training set. . test_df = pd.read_csv(&#39;test_features.tsv&#39;, delimiter=&#39; t&#39;) test_df.columns = [&#39;text&#39;] final_test_dataset = Dataset.from_pandas(test_df) final_test_dataset = final_test_dataset.map(tokenize, batched=True, batch_size=len(final_test_dataset)) final_test_dataset.set_format(&#39;torch&#39;, columns=[&#39;input_ids&#39;, &#39;attention_mask&#39;]) . . !wget https://raw.githubusercontent.com/ptaszynski/cyberbullying-Polish/master/task%2001/test_set_clean_only_tags.txt df_lbls = pd.read_csv(&#39;test_set_clean_only_tags.txt&#39;,names=[&#39;label&#39;]) labels = df_lbls.label.values . --2020-10-26 08:35:05-- https://raw.githubusercontent.com/ptaszynski/cyberbullying-Polish/master/task%2001/test_set_clean_only_tags.txt Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3000 (2.9K) [text/plain] Saving to: ‚Äòtest_set_clean_only_tags.txt‚Äô test_set_clean_only 100%[===================&gt;] 2.93K --.-KB/s in 0s 2020-10-26 08:35:05 (87.7 MB/s) - ‚Äòtest_set_clean_only_tags.txt‚Äô saved [3000/3000] . preds = trainer.predict(final_test_dataset) outputs = preds.predictions.argmax(axis=1) . . [32/32 00:10] precision, recall, f1, _ = precision_recall_fscore_support(labels, outputs, average=&#39;binary&#39;) acc = accuracy_score(labels, outputs) print( { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall }) . {&#39;accuracy&#39;: 0.913, &#39;f1&#39;: 0.5671641791044776, &#39;precision&#39;: 0.8507462686567164, &#39;recall&#39;: 0.4253731343283582} . The F1 score on the test set is around 0.56 - 0.59, which is in the range of state-of-the-art result last year during the PolEval 2019 competition. It is also pretty competitive on the KLEJ benchmark, although the models based on Roberta-large architecture perform better, and Polish Roberta base is also significantly better. In a separate post, we will try to reach those scores. . &lt;/div&gt; .",
            "url": "https://skok.ai/2020/10/27/Training-BERT-for-Cyberbullying-Detection-Part-1.html",
            "relUrl": "/2020/10/27/Training-BERT-for-Cyberbullying-Detection-Part-1.html",
            "date": " ‚Ä¢ Oct 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Top Down Introduction to BERT with HuggingFace and PyTorch",
            "content": "If you&#39;re just getting started with BERT, this article is for you. I will explain the most popular use cases, the inputs and outputs of the model, and how it was trained. I will also provide some intuition into how it works, and will refer your to several excellent guides if you&#39;d like to get deeper. . I&#39;ve spent the last couple of months working on different NLP tasks, including text classification, question answering, and named entity recognition. BERT has been my starting point for each of these use cases - even though there is a bunch of new transformer-based architectures, it still performs surprisingly well, as evidenced by the recent Kaggle NLP competitions. Eventually, I also ended up training my own BERT model for Polish language and was the first to make it broadly available via HuggingFace library. . Fortunately, you probably won&#39;t need to train your own BERT - pre-trained models are available for many languages, including several Polish language models published now. . HuggingFace and PyTorch . HuggingFace Transformers is an excellent library that makes it easy to apply cutting edge NLP models. I will use their code, such as pipelines, to demonstrate the most popular use cases for BERT. We will need pre-trained model weights, which are also hosted by HuggingFace. I will use PyTorch in some examples. . !pip install transformers -q . from transformers import pipeline, BertTokenizer, BertModel, BertForNextSentencePrediction, BertConfig import torch . What can I use BERT for? . Text classification . Probably the most popular use case for BERT is text classification. This means that we are dealing with sequences of text and want to classify them into discrete categories. . Here are some examples of text sequences and categories: . Movie Review - Sentiment: positive, negative | Product Review - Rating: one to five stars | Email - Intent: product question, pricing question, complaint, other | . Below is a code example of sentiment classification use case. . # Text classification - sentiment analysis nlp = pipeline(&quot;sentiment-analysis&quot;) print(nlp(&quot;This movie was great!&quot;)) print(nlp(&quot;I have just wasted 2 hours of my time.&quot;)) . [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.6986343860626221}] [{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9613907337188721}] . Named Entity Recognition . Sometimes, we&#39;re not interested in the overall text, but specific words in it. Maybe we want to extract the company name from a report. Or the start and end date of hotel reservation from an email. . That means that we need to apply classification at the word level - well, actually BERT doesn&#39;t work with words, but tokens (more on that later on), so let&#39;s call it token classification. . There are existing pre-trained models for common types of named entities, like people names, organization names or locations. Let&#39;s see how this performs on an example text. Note that we will only print out the named entities, the tokens classified in the &#39;Other&#39; category will be ommitted. . # NER / token classification nlp = pipeline(&quot;ner&quot;) sequence = &quot;My name is Darek and I live in Warsaw.&quot; for token in nlp(sequence): print(token) . {&#39;word&#39;: &#39;Dare&#39;, &#39;score&#39;: 0.9987152218818665, &#39;entity&#39;: &#39;I-PER&#39;} {&#39;word&#39;: &#39;##k&#39;, &#39;score&#39;: 0.9988871812820435, &#39;entity&#39;: &#39;I-PER&#39;} {&#39;word&#39;: &#39;Warsaw&#39;, &#39;score&#39;: 0.9978176355361938, &#39;entity&#39;: &#39;I-LOC&#39;} . Question Answering . Wouldn&#39;t it be great if we simply asked a question and got an answer? That is certainly a direction where some of the NLP research is heading (for example T5). BERT can only handle extractive question answering. It means that we provide it with a context, such as a Wikipedia article, and a question related to the context. BERT will find for us the most likely place in the article that contains an answer to our question, or inform us that an answer is not likely to be found. . # Question Answering nlp = pipeline(&quot;question-answering&quot;) context = &quot;My name is Darek. I&#39;m Polish. I like to practice kungfu. My home is in Warsaw but I often travel to Berlin. My friend, Paul, lives in Canada.&quot; print(nlp(question=&quot;Where does Darek live?&quot;, context=context)) print(nlp(question=&quot;Where does Paul live?&quot;, context=context)) . {&#39;score&#39;: 0.8502292525232313, &#39;start&#39;: 71, &#39;end&#39;: 77, &#39;answer&#39;: &#39;Warsaw&#39;} {&#39;score&#39;: 0.9584999083856722, &#39;start&#39;: 134, &#39;end&#39;: 140, &#39;answer&#39;: &#39;Canada.&#39;} . Other use cases and fine-tuning . There are some other interesting use cases for transformer-based models, such as text summarization, text generation, or translation. BERT is not designed to do these tasks specifically, so I will not cover them here. . The examples above are based on pre-trained pipelines, which means that they may be useful for us if our data is similar to what they were trained on. Very often, we will need to fine-tune a pretrained model to fit our data or task. This is much more efficient than training a whole model from scratch, and with few examples we can often achieve very good performance. . To be able to do fine-tuning, we need to understand a bit more about BERT. . What are the inputs to BERT, and what comes out of it? . Let&#39;s start by treating BERT as a black box. The minimum that we need to understand to use the black box is what data to feed into it, and what type of outputs to expect. You can build on top of these outputs, for example by adding one or more linear layers. You can then fine-tune your custom architecture on your data. . Tokenization . Before you feed your text into BERT, you need to turn it into numbers. That&#39;s the role of a tokenizer. Some tokenizers split text on spaces, so that each token corresponds to a word. That would result however in a huge vocabulary, which makes training a model more difficult, so instead BERT relies on sub-word tokenization. Let&#39;s see how it works in code. . Each pre-trained model comes with a pre-trained tokenizer (we can&#39;t separate them), so we need to download it as well. Let&#39;s use it then to tokenize a line of text and see the output. . tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) . text = &#39;I like to practice kungfu.&#39; tokens = tokenizer.encode(text) print(tokens) . [101, 1045, 2066, 2000, 3218, 18577, 11263, 1012, 102] . Each token is a number that corresponds to a word (or subword) in the vocabulary. The most frequent words are represented as a whole word, while less frequent words are divided in sub-words. That ensures that we can map the entire corpus to a fixed size vocabulary without unknown tokens (in reality, they may still come up in rare cases). Let&#39;s see the length of our model&#39;s vocabulary, and how the tokens corresponds to words. . print(f&#39;Length of BERT base vocabulary: {len(tokenizer.vocab)}&#39;) print(f&#39;Text: {text}&#39;) for t in tokens: print(f&#39;Token: {t}, subword: {tokenizer.decode([t])}&#39;) . Length of BERT base vocabulary: 30522 Text: I like to practice kungfu. Token: 101, subword: [CLS] Token: 1045, subword: i Token: 2066, subword: like Token: 2000, subword: to Token: 3218, subword: practice Token: 18577, subword: kung Token: 11263, subword: ##fu Token: 1012, subword: . Token: 102, subword: [SEP] . In the example, you can see how the tokenizer split a less common word &#39;kungfu&#39; into 2 subwords: &#39;kung&#39; and &#39;##fu&#39;. The &#39;##&#39; characters inform us that this subword occurs in the middle of a word. BERT tokenizer also added 2 special tokens for us, that are expected by the model: [CLS] which comes at the beginning of every sequence, and [SEP] that comes at the end. [SEP] may optionally also be used to separate two sequences, for example between question and context in a question answering scenario. Another example of a special token is [PAD], we need to use it to pad shorter sequences in a batch, because BERT expects each example in a batch to have the same amount of tokens. . Outputs . Let&#39;s download a pretrained model now, run our text through it, and see what comes out. We will first need to convert the tokens into tensors, and add the batch size dimension (here, we will work with batch size 1). . model = BertModel.from_pretrained(&#39;bert-base-uncased&#39;) . inputs = torch.tensor(tokens).unsqueeze(0) # Batch size 1 outputs = model(inputs) print(f&#39;output type: {type(outputs)}, output length: {len(outputs)}&#39;) print(f&#39;first item shape: {outputs[0].shape}&#39;) print(f&#39;second item shape: {outputs[1].shape}&#39;) . output type: &lt;class &#39;tuple&#39;&gt;, output length: 2 first item shape: torch.Size([1, 9, 768]) second item shape: torch.Size([1, 768]) . In the examples above, we used BERT to handle some useful tasks, such as text classification, named entity recognition, or question answering. For each of those tasks, a task-specific model head was added on top of raw model outputs. Here, we are dealing with the raw model outputs - we need to understand them to be able to add custom heads to solve our own, specific tasks. . The model outputs a tuple. The first item of the tuple has the following shape: 1 (batch size) x 9 (sequence length) x 768 (the number of hidden units). This is called the sequence output, and it provides the representation of each token in the context of other tokens in the sequence. If we&#39;d like to fine-tune our model for named entity recognition, we will use this output and expect the 768 numbers representing each token in a sequence to inform us if the token corresponds to a named entity. . The second item in the tuple has the shape: 1 (batch size) x 768 (the number of hidden units). It is called the pooled output, and in theory it should represent the entire sequence. It corresponds to the first token in a sequence (the [CLS] token). We can use it in a text classification task - for example when we fine-tune the model for sentiment classification, we&#39;d expect the 768 hidden units of the pooled output to capture the sentiment of the text. . In practice, we may want to use some other way to capture the meaning of the sequence, for example by averaging the sequence output, or even concatenating the hidden states from lower levels. . How was BERT trained? . The models we have been using so far have already been pre-trained, and in some cases fine-tuned as well. What does this actually mean? . Pre-training . In order for a model to solve an NLP task, like sentiment classification, it needs to understand a lot about language. Most of the labelled datasets that we have available are too small to teach our model enough about language. Ideally, we&#39;d like to use all the text we have available, for example all books and the internet. Because it&#39;s hard to label so much text, we create &#39;fake tasks&#39; that will help us achieve our goal without manual labelling. . BERT is trained on a very large corpus using two &#39;fake tasks&#39;: masked language modeling (MLM) and next sentence prediction (NSP). In MLM, we randomly hide some tokens in a sequence, and ask the model to predict which tokens are missing. In NSP, we provide our model with two sentences, and ask it to predict if the second sentence follows the first one in our corpus. The intent of these tasks is for our model to be able to represent the meaning of both individual words, and the entire sentences. . nlp = pipeline(&quot;fill-mask&quot;) preds = nlp(f&quot;I am exhausted, it&#39;s been a very {nlp.tokenizer.mask_token} day.&quot;) print(&#39;I am exhausted, it &#39;s been a very ***** day.&#39;) for p in preds: print(nlp.tokenizer.decode([p[&#39;token&#39;]])) preds = nlp(f&quot;I am excited, it&#39;s been a very {nlp.tokenizer.mask_token} day.&quot;) print(&#39;I am excited, it &#39;s been a very ***** day.&#39;) for p in preds: print(nlp.tokenizer.decode([p[&#39;token&#39;]])) . I am exhausted, it&#39;s been a very ***** day. busy exhausting stressful taxing rough I am excited, it&#39;s been a very ***** day. busy exciting productive good nice . tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;) model = BertForNextSentencePrediction.from_pretrained(&#39;bert-base-uncased&#39;) first_sentence = &quot;I cut my finger.&quot; second_sentence_right = &quot;The blood started flowing.&quot; second_sentence_wrong = &quot;This website uses cookies.&quot; right = tokenizer.encode_plus(first_sentence, text_pair=second_sentence_right) wrong = tokenizer.encode_plus(first_sentence, text_pair=second_sentence_wrong) r1, r2, r3 = torch.tensor(right[&#39;input_ids&#39;]).unsqueeze(0), torch.tensor(right[&#39;token_type_ids&#39;]).unsqueeze(0), torch.tensor(right[&#39;attention_mask&#39;]).unsqueeze(0) w1, w2, w3 = torch.tensor(wrong[&#39;input_ids&#39;]).unsqueeze(0), torch.tensor(wrong[&#39;token_type_ids&#39;]).unsqueeze(0), torch.tensor(wrong[&#39;attention_mask&#39;]).unsqueeze(0) right_outputs = model(input_ids=r1, token_type_ids=r2, attention_mask=r3) right_seq_relationship_scores = right_outputs[0] wrong_outputs = model(input_ids=w1, token_type_ids=w2, attention_mask=w3) wrong_seq_relationship_scores = wrong_outputs[0] print(first_sentence + &#39; &#39; + second_sentence_right) print(f&#39;Next sentence prediction: {right_seq_relationship_scores.detach().numpy().flatten()[0] &gt; 0}&#39;) print(first_sentence + &#39; &#39; + second_sentence_wrong) print(f&#39;Next sentence prediction: {wrong_seq_relationship_scores.detach().numpy().flatten()[0] &gt; 0}&#39;) . I cut my finger. The blood started flowing. Next sentence prediction: True I cut my finger. This website uses cookies. Next sentence prediction: False . Finetuning . As we can see from the examples above, BERT has learned quite a lot about language during pretraining. That knowledge is represented in its outputs - the hidden units corresponding to tokens in a sequence. We can use that knowledge by adding our own, custom layers on top of BERT outputs, and further training (finetuning) it on our own data. . How does BERT really work? . If training a model is like training a dog, then understanding the internals of BERT is like understanding the anatomy of a dog. It&#39;s not required to effectively train a model, but it can be helpful if you want to do some really advanced stuff, or if you want to understand the limits of what is possible. . I will only scratch the surface here by showing the key ingredients of BERT architecture, and at the end I will point to some additional resources I have found very helpful. . Let&#39;s start by loading up basic BERT configuration and looking what&#39;s inside. . config = BertConfig() config . BertConfig { &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;layer_norm_eps&#34;: 1e-12, &#34;max_position_embeddings&#34;: 512, &#34;model_type&#34;: &#34;bert&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 0, &#34;type_vocab_size&#34;: 2, &#34;vocab_size&#34;: 30522 } . This configuration file lists the key dimensions that determine the size of the model: . 768 hidden size is the number of floats in a vector representing each token in the vocabulary | 30522 is the vocabulary size | We can deal with max 512 tokens in a sequence | The initial embeddings will go through 12 layers of computation, including the application of 12 attention heads and dense layers with 3072 hidden units, to produce our final output, which will again be a vector with 768 units per token | . Let&#39;s briefly look at each major building block of the model architecture. We start with the embedding layer, which maps each vocabulary token to a 768-long embedding. We can also see position embeddings, which are trained to represent the ordering of words in a sequence, and token type embeddings, which are used if we want to distinguish between two sequences (for example question and context). . model = BertModel(config) print(model.embeddings) . BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) . Then, we pass the embeddings through 12 layers of computation. This starts with self-attention, is followed by an intermediate dense layer with hidden size 3072, and ends with sequence output that we have already seen above. Usually, we will deal with the last hidden state, i.e. the 12th layer. However, to achieve better results, we may sometimes use the layers below as well to represent our sequences, for example by concatenating the last 4 hidden states. . print(f&#39;There are {len(model.encoder.layer)} layers like this in the model architecture:&#39;) print(&#39;&#39;) print(model.encoder.layer[0]) . There are 12 layers like this in the model architecture: BertLayer( (attention): BertAttention( (self): BertSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) . Finally, we have the pooled output, which is used in pre-training for the NSP task, and corresponds to the [CLS] token hidden state that goes through another linear layer. . print(model.pooler) . BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) . In this overview, I haven&#39;t explained at all the self-attention mechanism, or the detailed inner workings of BERT. If you&#39;d like to learn further, here are some materials that I have found very useful. . Chris Mccormick BERT Research Series on Youtube | Jay Alammar A Visual Guide to Using BERT for the First Time | Jay Alammar The Illustrated Transformer | Peter Bloem Transformers from Scratch | .",
            "url": "https://skok.ai/2020/05/11/Top-Down-Introduction-to-BERT.html",
            "relUrl": "/2020/05/11/Top-Down-Introduction-to-BERT.html",
            "date": " ‚Ä¢ May 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "My Transition Into Data Science",
            "content": "For the last couple of years, I‚Äôve been fascinated by the progress of machine learning, and the potential applications it enables. I started a journey of learning, building, researching and designing machine learning applications. In this post, I will share my approach and plans for the future. . Getting started . I started learning a few years ago with the classic Coursera Machine Learning course by Andrew Ng. It got me hooked, stirred up my interest in machine learning, provided an intuition into the way it works, but left largely unable to do anything practical about it - if only for the reason that it was based on Octave. I needed a practical challenge, so after my daughter was born, I planned to build an app that would generate nursery rhymes in Polish. I went through the Deep Learning specialisation on Coursera, again by Andrew Ng, which was much more hands on and practical. I experimented with LSTMs to generate the rhymes, found out a lot of practical limitations around machine learning, and ultimately shipped the app - although in the background it used an N-gram based language model, a much more traditional technique. . Another breakthrough in my personal journey was discovering Fast.ai. The courses, the library, the community, and specifically the advice from Fast.ai crew: Jeremy Howard, Rachel Thomas, and Sylvain Gugger, gave me a whole new level of practical skills, understanding and motivation to apply machine learning. It keeps amazing me how we can learn from some of the best teachers in the world, and I‚Äôm very grateful for that. . Practice, practice, practice . Outside of my family and machine learning, my great passion is kung fu, so I‚Äôm used to regular practice and continuous improvement. In a typical sifu/sensei fashion, Jeremy Howard keeps advising to practice machine learning, experiment with code, develop applications, join Kaggle competitions. For me, Kaggle in particular was a huge boost to my skill level and confidence. I got to develop solutions to practical business problems, including localisation of steel defects, fraud detection, natural question answering and many more. Kaggle platform provides a great way to get started, via starter notebooks and solutions shared by the community, and it provides a way to keep learning and improving by analyzing the final solutions shared by the winning teams. Getting ranked on Kaggle is also a very good credential, in particular for people like me who moved into data science from a different field. . Applications . I‚Äôm lucky that my job, which is about identifying and executing ideas to transform business processes with technology, allows me to benefit from my machine learning skills and apply them. I‚Äôve gradually transitioned from a pure manager role to half-manager, half-hands-on practitioner, which I feel results in a higher ROI on my salary and is also much more interesting. Applying machine learning in a big company is subject to multiple constraints though (which is a very sensible approach). I‚Äôm compensating for this by working on more ‚Äòcrazy‚Äô projects outside of work, which includes some research and some potentially commercial applications, that I‚Äôm hoping to reveal in the future. . Blogging . Rachel Thomas wrote a great blog post about blogging. I feel like I‚Äôm already late to get this started, but it‚Äôs still better to do it now. I‚Äôll keep sharing my learnings, insights, and projects on this blog. I‚Äôm especially interested in applying NLP to Polish language, and related applications, so if you‚Äôre interested in these topics, do follow me on Twitter! .",
            "url": "https://skok.ai/learning/applications/2020/05/09/discovering-the-potential-of-machine-learning.html",
            "relUrl": "/learning/applications/2020/05/09/discovering-the-potential-of-machine-learning.html",
            "date": " ‚Ä¢ May 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Darek K≈Çeczek, and I like to solve practical problems with machine learning. . Working on Intelligent Automation at P&amp;G | Married, with a 3-year old daughter | ML / Kaggle / Kung Fu in the remaining time | Online educated in ML: Andrew Ng / Coursera | Jeremy Howard, Rachel Thomas, Sylvain Gugger / Fast.ai | . | Projects: mainly Polish NLP Generate nursery rhymes in Polish | Compare press coverage from different sources (not published yet) | Polish BERT (first publicly shared BERT model for Polish) | PolEval 2020 - solutions to 3 out of 4 competition tasks (article) | Polish NLP Meetup Group | . | . Attributions: Favicon made by Freepik from www.flaticon.com .",
          "url": "https://skok.ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://skok.ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}